---
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
options(scipen = 999, digits = 3)
```

## Introdução

Esse repositório contém códigos do desafio Zindi Africa. Aqui, é descrito todo o processo para chegar no resultado final. Os códigos completos do estudo estão no arquivo `CaseAnaliseDados.R`. Os dados, obtidos de https://zindi.africa/competitions/financial-inclusion-in-africa/data estão na pasta `data`.

O objetivo é fazer um modelo que identifique quais pessoas possuem conta no banco, correspondente a variável `bank_account`.

O desafio foi feito na linguagem `R` e as etapas do processo são:

- Análise exploratória dos dados
- Balanceamento de variáveis
- Modelagem
- Resultados

## Importações

Os pacotes `data.table` e `tidyverse` (que inclui `dplyr`, `tidyr` e outros) serão utilizados para as operações de *data wrangling*. Para as visualizações, utilizarei o `ggplot2` (incluído no `tidyverse`) e `magick`; enquanto que os pacotes `caret`, `pROC` e `xgboost` serão utilizados na parte de *machine learning*.

Os dados originais estão divididos entre treino e teste, sendo que o teste não contém a variável resposta. 

## Análise Exploratória dos Dados

```{r echo=FALSE, warning=F, message=F}
library(data.table)
library(tidyverse)
library(magick)
library(caret)
library(pROC)
library(xgboost)

ag_colors <- c('#3e6dbe', '#78ca35', '#253165')

vars <- fread('data/VariableDefinitions.csv', header = T, encoding = 'UTF-8')
train <- fread('data/Train_v2.csv')
test <- fread('data/Test_v2.csv')

set.seed(123)
```


``` {r}

names(train)

```


```{r, echo=FALSE}

logo <- image_read("https://logospng.org/download/agibank/logo-agibank-icon-2048.png")

perc_bank <- train[, .(n = .N), bank_account][, perc := round(n / sum(n), 2)][]

p1 <- ggplot(perc_bank, aes(x = bank_account, y = n, fill = bank_account)) +
  geom_col(position = "dodge") +
  geom_text(
    aes(x = bank_account, y = n, label = paste0(perc * 100, '%')),
    position = position_dodge(width = 1),
    vjust = -0.5, size = 4
  ) + 
  scale_fill_manual(values = ag_colors) + 
  labs(title = 'DESBALANÇO ENTRE AS VARIÁVEIS RESPOSTA',
       subtitle = 'CONTAGEM POR CATEGORIA',
       fill = 'CONTA NO BANCO') +
  theme_minimal() +
  theme(axis.title = element_blank())

p1
grid::grid.raster(logo, x = .98, y = 0.015, just = c('right', 'bottom'), width = unit(.3, 'inches'))

```

Há um desbalanço na variável resposta. Isso deve ser levado em consideração na hora de treinar o modelo, pois datasets desbalanceados podem influenciar no resultado do modelo.


```{r, echo=FALSE}

perc_country <- train[, .(n = .N), by = .(country, bank_account)][, perc := round( n / sum(n), 2), country][]

p2 <- ggplot(perc_country, aes(x = country, 
                               y =  n,
                               fill = bank_account)) +
  geom_col(position = "dodge") +
  geom_text(
    aes(x = country, y = n, label = paste0(perc *100, '%')),
    position = position_dodge(width = 1),
    vjust = -0.5, size = 4
  ) + 
  scale_fill_manual(values = ag_colors) + 
  labs(title = 'Contagem de pessoas que possuem conta no banco por País',
       fill = 'Conta no banco') +
  theme_minimal() +
  theme(axis.title = element_blank(), 
        plot.title = element_text(size = 16))

p2
grid::grid.raster(logo, x = .98, y = 0.015, just = c('right', 'bottom'), width = unit(.3, 'inches'))


```

Os países apresentam taxas diferentes de pessoas com contas bancárias. 

## Modelagem

O primeiro passo a ser feito é rodar um modelo de *machine learning* e vermos os resultados iniciais obtidos, para então tomarmos a decisão de que caminho seguir. Como não temos a variável resposta no dataset de teste, iremos criar um dataset de validação a partir do dataset de treino, que ficará de fora do treinamento do nosso modelo, para que possamos avaliar como ele está performando.

O pacote `caret` tem uma função que facilita o processo de dividir o dataset de maneira distribuida como o dataset original. 75% do dataset original permaneceu no dataset de treino, e 25% ficou separado no de validação.

``` {r}

trainIndex <- createDataPartition(train$bank_account, p = .75, 
                                  list = FALSE, times = 1)

validation <- train[-trainIndex]
train <- train[trainIndex]

y_train <- as.factor(train$bank_account)
y_validation <- as.factor(validation$bank_account)

train <- train[, -'year', with=F]
validation <- validation[, -'year', with=F]


```


Para treinar nosso primeiro modelo, utilizaremos a metodologia *Random Forest*, modelo não-paramétrico que se adapta bem a diversos tipos de dataset. Ele utiliza apenas um hiperparâmetro, o `mtry`. Uma regra de bolso é utilizar a raiz quadrada do número de variáveis do dataset para o `mtry`. Também utilizaremos o método de *Cross Validation*.

``` {r eval = F}

trcontrol = trainControl( method = "cv",
                          number = 5,  
                          allowParallel = TRUE,
                          verboseIter = TRUE )

mtry <- sqrt(ncol(train[, -'bank_account', with=F]))
tunegrid <- expand.grid(.mtry=mtry)

rf_model <- train(x = train[, -'bank_account', with=F], 
                  y = y_train,
                  trControl = trcontrol,
                  tuneGrid = tunegrid,
                  method = "rf")

```

Depois de rodarmos o modelo, iremos avaliar a sua performance. Para isso, iremos comparar a acurácia das previsões tanto no treino de teste, quando no de validação, que separamos apenas para isso. Se a performance no treino de validação não for muito inferior a de teste, quer dizer que nosso modelo pode performar bem em dados futuros, não apresentando *overfit*.

``` {r echo = F}

cm_rf_train <-          readRDS('data/outputs/cm_rf_train.rds')
cm_rf_validation <-     readRDS('data/outputs/cm_rf_validation.rds')
cm_rf_ds_train <-       readRDS('data/outputs/cm_rf_ds_train.rds')
cm_rf_ds_validation <-  readRDS('data/outputs/cm_rf_ds_validation.rds')
cm_glm_validation <-    readRDS('data/outputs/cm_glm_validation.rds')
cm_glm_train <-         readRDS('data/outputs/cm_glm_train.rds')
cm_xgb_train <-         readRDS('data/outputs/cm_xgb_train.rds')
cm_validation_xgb <-    readRDS('data/outputs/cm_validation_xgb.rds')

```

### Matriz de Confusão do treino

``` {r echo = F}

cm_rf_train

```

### Matriz de Confusão da validação

``` {r echo = F}

cm_rf_validation

```

No dataset de treino, obtivemos uma acurácia de 0.933, e no de validação 0.883. Se fosse só por essa informação, poderíamos dizer que nosso modelo está performando bem. Porém, pelos valores de especificidade, vemos que o fato de o dataset estar desbalanceado pode estar impactando o resultado de nossas previsões. 

## Downsamplig

Dessa maneira, utilizaremos o método de *Downsampling* para obter um dataset que tenha 50% de usuários com conta em banco e 50% de usuários sem conta em banco. Datasets assim tendem a nos dar modelos que capturem melhor as nuances de ambos os valores. Para isso, mais uma vez utilizamos o pacote `caret`.

Apenas uma linha de comando e temos nosso novo dataset, agora balanceado.

``` {r}

train_ds <- downSample(train, y_train, yname = 'bank_account')
setDT(train_ds)[, .N, bank_account]

```

Rodamos novamente o modelo de *Random Forest*, dessa vez no dataset balanceado o analisamos seus resultados.

``` {r eval = F}

y_ds <- train_ds$bank_account
train_ds <- train_ds[, -'bank_account', with=F]

rf_ds_model <- train(x = train_ds, 
                     y = y_ds,
                     trControl = trcontrol,
                     tuneLength = tunegrid,
                     method = "rf")

```

### Matriz de Confusão do treino balanceado

``` {r echo = F}

cm_rf_ds_train

```

### Matriz de Confusão do modelo com downsampling aplicado na validação

``` {r echo = F}

cm_rf_ds_validation

```


Ainda que tenhamos perdido um pouco de acurácia em relação ao modelo original, o novo modelo, feito a partir do dataset balanceado com a tecnica de *downsampling*, nos dá resultados mais equilibrados, que podemos ver pela Sensibilidade e Especificidade.

Nota: É possível utilizar também o *upsampling*, que balanceia o dataset com mais observações, copiando observações da variável com menos entradas. Aqui, utilizei apenas o *downsampling*, mas no script de código completo, faço a avaliação dos dois métodos.

## Modelagem

Agora, iremos testar outras metodologias, como o simples *GLM* e o *XGBoost*.

### GLM

```{r eval = F}

glm_model <- train( x = train_ds, 
                    y = y_ds,
                    trControl = trcontrol,
                    tuneLength = 5,
                    method = "glm")

```

### Matriz de Confusão do GLM no treino balanceado

```{r echo = F}

cm_glm_train

```

### Matriz de Confusão do GLM na validação

```{r echo = F}

cm_glm_validation

```

### xGBoost

Para rodarmos o modelo de *xGBoost*, precisamos de algumas transformações antes. O algoritmo do modelo necessita que cada variável categórica esteja como numérica, como se fossem *dummies*. Assim, usamos o processo chamado *One Hot Encoding*. O pacote `caret` nos ajuda mais uma vez.

``` {r eval = F}

dmy_train_ds <- dummyVars('~.', data = train_ds)
train_matrix_ds <- data.table(predict(dmy_train_ds, newdata = train_ds))
train_matrix_ds <- train_matrix_ds %>% as.matrix() %>% xgb.DMatrix()


dmy_validation <- dummyVars('~.', data = validation[, -'bank_account', with = F])
validation_matrix <- data.table(predict(dmy_validation, newdata = validation[, -'bank_account', with = F]))
validation_matrix <- validation_matrix %>% as.matrix() %>% xgb.DMatrix()

```

```{r eval = F}

xgb_model <- train( x = train_matrix_ds, 
                    y = y_ds,
                    trControl = trcontrol,
                    tuneLength = 4,
                    method = "xgbTree")

```

### Matriz de Confusão do xGBoost no treino balanceado

```{r echo = F}

cm_xgb_train

```

### Matriz de Confusão do xGBoost no treino balanceado

```{r echo = F}

cm_validation_xgb

```

## Conclusão

Os resultados dos 3 modelos foram bem parecidos, dessa maneira, entendo que temos mais vantagens ao escolhermos o mais simples, no caso o **GLM**. Isso porque ele nos dá parâmetros, dessa maneira podemos avaliar o impacto de cada variável na previsão. Datasets desbalanceados são bastante frequentes em casos de detecção de fraude ou detecção de câncer, por exemplo. O método de *downsampling*, utilizado aqui, pode ajudar a corrigir algum viés que esse desbalanço pode produzir nos modelos.

O arquivo `SubmissionFile.csv` está preenchido com as previsões do teste, no padrão proposto no arquivo original.



